{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, ConcatDataset, SubsetRandomSampler\n",
    "from torchvision.transforms import ToTensor, Compose\n",
    "from torchvision.transforms import ToTensor, Compose, Normalize, Resize, ConvertImageDtype\n",
    "from torchvision.transforms import RandomHorizontalFlip, RandomPerspective, \\\n",
    "    ColorJitter, GaussianBlur, RandomAffine\n",
    "import torch\n",
    "from transformers import BeitForImageClassification\n",
    "from PIL import Image\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO FIX DOWNLOADING ERRORS \n",
    "import requests\n",
    "import ssl\n",
    "\n",
    "requests.packages.urllib3.disable_warnings()\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    # Legacy Python that doesn't verify HTTPS certificates by default\n",
    "    pass\n",
    "else:\n",
    "    # Handle target environment that doesn't support HTTPS verification\n",
    "    ssl._create_default_https_context = _create_unverified_https_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_labels(df):\n",
    "    # Total of 29996 samples in train set\n",
    "    # Range 1-19 (12 missing)\n",
    "    # 18 classes\n",
    "    # output one-hot vector (18-dim) with at least one \"1\"\n",
    "    labels = df[\"Labels\"]\n",
    "    num_samples = len(labels)\n",
    "    num_classes = 18\n",
    "    Y_data = np.zeros((num_samples, num_classes))\n",
    "    for i in range(len(labels)):\n",
    "        label = labels[i]\n",
    "        label_ls = label.split(\" \")\n",
    "        label_ints = [int(label_str) for label_str in label_ls]\n",
    "        for idx in label_ints:\n",
    "            if idx >= 12:\n",
    "                idx -= 1\n",
    "            Y_data[i][idx-1] = 1  \n",
    "            \n",
    "    Y_data = Y_data.reshape((29996, 1, 1, 18))\n",
    "    return Y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_labels():\n",
    "    try:\n",
    "        FILENAME = \"./COMP5329S1A2Dataset/train.csv\"\n",
    "        with open(FILENAME) as file:\n",
    "            lines = [re.sub(r'([^,])\"(\\s*[^\\n])', r'\\1/\"\\2', line) for line in file]\n",
    "            df = pd.read_csv(StringIO(''.join(lines)), escapechar=\"/\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {FILENAME} not found\")\n",
    "        exit()\n",
    "\n",
    "    Y_data = extract_labels(df)\n",
    "    return Y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_ids(train):\n",
    "    try:\n",
    "        if train:\n",
    "            FILENAME = \"./COMP5329S1A2Dataset/train.csv\"\n",
    "        else:\n",
    "            FILENAME = \"./COMP5329S1A2Dataset/test.csv\"\n",
    "        with open(FILENAME) as file:\n",
    "            lines = [re.sub(r'([^,])\"(\\s*[^\\n])', r'\\1/\"\\2', line) for line in file]\n",
    "            df = pd.read_csv(StringIO(''.join(lines)), escapechar=\"/\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {FILENAME} not found\")\n",
    "        exit()\n",
    "\n",
    "    # There are actually 30000 images in the train folder\n",
    "    # but we only take a select number of them due to \n",
    "    # train_df only having 29996 entries\n",
    "    img_ids = df[\"ImageID\"]\n",
    "    return img_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalDataset(Dataset):\n",
    "    def __init__(self, train, img_transform=None, target_transform=None):\n",
    "        self.train = train\n",
    "\n",
    "        self.img_ids = get_img_ids(train=train)\n",
    "\n",
    "        if train:\n",
    "            self.labels = get_train_labels()\n",
    "        else:\n",
    "            self.labels = None\n",
    "        \n",
    "        self.img_transform = img_transform\n",
    "        self.target_transform = target_transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.train:\n",
    "            file_path = \"./COMP5329S1A2Dataset/data/train/\" + self.img_ids[idx]\n",
    "        else:\n",
    "            file_path = \"./COMP5329S1A2Dataset/data/test/\" + self.img_ids[idx]\n",
    "        img = Image.open(file_path)\n",
    "        \n",
    "        if self.img_transform is not None:\n",
    "            img = self.img_transform(img)\n",
    "        \n",
    "        if self.labels is not None:\n",
    "            label = self.labels[idx]        \n",
    "            label = self.target_transform(label)\n",
    "            label = label.view(18).type(torch.float32) \n",
    "        else:\n",
    "            label = -1\n",
    "            \n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_weight(dataset, device):\n",
    "    assert len(dataset.datasets) == 2\n",
    "    train_labels = dataset.datasets[0].labels.repeat(2, axis=0)\n",
    "\n",
    "    # Normalized inverse frequency of occurrences of labels\n",
    "    classes = train_labels.sum(axis=0)\n",
    "    all_samples = train_labels.sum()\n",
    "    inv_classes = all_samples / classes\n",
    "    min = np.min(inv_classes); max = np.max(inv_classes)\n",
    "    lower_bound = 1; upper_bound = 10\n",
    "    inv_classes_norm = (inv_classes-min)/(max-min) * (upper_bound-lower_bound) + lower_bound \n",
    "    pos_weight = torch.from_numpy(inv_classes_norm)\n",
    "\n",
    "    return pos_weight.type(torch.float32).view(18).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, device, criterion, mode=\"img\"):\n",
    "    loss = 0\n",
    "    corr = 0\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in dataloader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            output = model(imgs).logits\n",
    "            preds = torch.round(torch.sigmoid(output))\n",
    "\n",
    "            loss += criterion(output, labels) * imgs.size(0)\n",
    "            corr += preds.eq(labels).all(dim=1).sum() # checks whether all samples equal\n",
    "\n",
    "    loss = loss.item() / len(dataloader.dataset)\n",
    "    acc = corr.item() / len(dataloader.dataset)\n",
    "    \n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = Compose([\n",
    "    Resize((224, 224), interpolation=3),\n",
    "    RandomHorizontalFlip(p=0.5),\n",
    "    RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "    RandomPerspective(p=0.5, distortion_scale=0.5, interpolation=3),\n",
    "    ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n",
    "    GaussianBlur(kernel_size=23, sigma=(0.1, 2.0)),\n",
    "    ToTensor(),\n",
    "    ConvertImageDtype(torch.float32),\n",
    "    Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "test_transform = Compose([\n",
    "    Resize((224, 224), interpolation=3),\n",
    "    ToTensor(),\n",
    "    ConvertImageDtype(torch.float32),\n",
    "    Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train, val, test datasets - 49992, 10000, 10000 samples\n",
    "train_set_normal = MultimodalDataset(train=True, img_transform=test_transform, target_transform=ToTensor())\n",
    "train_set_augmented = MultimodalDataset(train=True, img_transform=train_transform, target_transform=ToTensor())\n",
    "train_set_combined = ConcatDataset([train_set_normal, train_set_augmented])\n",
    "test_set = MultimodalDataset(train=False, img_transform=test_transform)\n",
    "\n",
    "batch_size = 512\n",
    "dataset_size = len(train_set_combined)\n",
    "train_size = int((1-len(test_set)/dataset_size) * dataset_size) \n",
    "\n",
    "indices = list(range(dataset_size))\n",
    "np.random.shuffle(indices) \n",
    "train_indices, val_indices = indices[:train_size], indices[train_size:]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = DataLoader(train_set_combined, batch_size=batch_size, sampler=train_sampler) # TODO: how to add shuffle\n",
    "val_loader = DataLoader(train_set_combined, batch_size=batch_size, sampler=val_sampler)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    type_device = \"cuda\"\n",
    "elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    type_device = \"mps\"\n",
    "else:\n",
    "    type_device = \"cpu\"\n",
    "    \n",
    "# type_device = \"cpu\" # might need to override, depending on computer\n",
    "print(f\"Using {type_device} device\")\n",
    "device = torch.device(type_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/microsoft/beit-base-patch16-224-pt22k-ft22k\n",
    "\n",
    "model = BeitForImageClassification.from_pretrained('microsoft/beit-base-patch16-224-pt22k-ft22k')\n",
    "model = model.to(device)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Linear(in_features=768, out_features=256), # ie. 512\n",
    "    nn.BatchNorm1d(num_features=256),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(in_features=256, out_features=128),\n",
    "    nn.BatchNorm1d(num_features=128),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(in_features=128, out_features=64),\n",
    "    nn.BatchNorm1d(num_features=64),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(in_features=64, out_features=18)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001 # before 1e-3\n",
    "num_epochs = 100\n",
    "losses = []\n",
    "best_loss = 1e7\n",
    "best_model = None\n",
    "\n",
    "pos_weight = get_pos_weight(train_set_combined, device)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"./best_model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "for epoch in range(num_epochs):  \n",
    "    print(f\"Epoch {epoch}\")\n",
    "    \n",
    "    for batch, (imgs, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        outputs = model(imgs).logits\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item()) \n",
    "        if batch % 10 == 0:\n",
    "            num_batches = len(train_loader)\n",
    "            print('batch: [%d/%d], loss: %.5f' %(batch, num_batches, loss.item()))           \n",
    "            \n",
    "    val_loss, val_acc = evaluate(model, val_loader, device, criterion)\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        best_model = copy.deepcopy(model)\n",
    "        torch.save(best_model.state_dict(), f\"./saved/model_{epoch}_{val_loss:.4f}_{val_acc:.4f}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./saved_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.title(\"Multimodal Network (ResNet18 + BiLSTM)\")\n",
    "plt.xlabel(\"Batch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "train_loss, train_acc = evaluate(model, train_loader, device, criterion)\n",
    "val_loss, val_acc = evaluate(model, val_loader, device, criterion)\n",
    "print(f\"train loss: {train_loss}\\ttrain acc: {train_acc}\")\n",
    "print(f\"val loss: {val_loss}\\tval acc: {val_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "     # shape (40, 256, 18) except for last batch which has shape (16, 18)\n",
    "    print(f\"Num batches: {len(test_loader)}\")\n",
    "    for batch, (imgs, _) in enumerate(test_loader):\n",
    "        print(f\"Batch {batch}\")\n",
    "        imgs = imgs.to(device)\n",
    "        output = model(imgs).logits\n",
    "        preds = torch.round(torch.sigmoid(output))\n",
    "        outputs.append(preds) \n",
    "saved_outputs = copy.deepcopy(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = torch.zeros((batch_size,18))\n",
    "remaining = outputs[0].shape[0] % batch_size\n",
    "# can replace \"remaining\" directly with 272 (if batch_size=512) or 16 (if batch_size=256)\n",
    "temp[:remaining] = outputs.pop() \n",
    "outputs.append(temp.to(device)) \n",
    "new_outputs = torch.stack(outputs, dim=0) # turn into tensor \n",
    "new_outputs = new_outputs.view(-1, 18) \n",
    "new_outputs = new_outputs[:10000] # now shape (10000, 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sums = new_outputs.sum(axis=0)\n",
    "for i in range(18):\n",
    "    print(f\"label: {i+1}\\tnum preds: {sums[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_strs = []\n",
    "for sample in new_outputs:\n",
    "    idxs = torch.where(sample == 1)[0]\n",
    "    idxs_ls = [str(idx.item()+1) for idx in idxs] # has strs \"1\"-\"18\"\n",
    "    for i in range(len(idxs_ls)):\n",
    "        idx = int(idxs_ls[i])\n",
    "        if idx >= 12: # shift 7 numbers over by 1 to right (since 12 has no labels)\n",
    "            idxs_ls[i] = str(idx+1)\n",
    "    label_strs.append(\" \".join(idxs_ls))\n",
    "label_strs = np.array(label_strs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENAME = \"./COMP5329S1A2Dataset/test.csv\"\n",
    "with open(FILENAME) as file:\n",
    "    lines = [re.sub(r'([^,])\"(\\s*[^\\n])', r'\\1/\"\\2', line) for line in file]\n",
    "    test_df = pd.read_csv(StringIO(''.join(lines)), escapechar=\"/\")\n",
    "if \"Caption\" in test_df.columns:\n",
    "    test_df = test_df.drop(\"Caption\", axis=1)\n",
    "test_df[\"Labels\"] = label_strs\n",
    "test_df.to_csv(\"./model_eval.csv\", sep=\",\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
